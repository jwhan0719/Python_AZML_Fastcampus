{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Use Naive Bayes to Classify Spam Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:09.530170Z",
     "start_time": "2022-04-01T01:23:08.069123Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data\n",
    "- 문자 내용이 스팸인지 아닌지를 구분하기 위한 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:09.592081Z",
     "start_time": "2022-04-01T01:23:09.536076Z"
    }
   },
   "outputs": [],
   "source": [
    "spam = pd.read_csv(\"../02. Data/sms_spam.csv\")\n",
    "\n",
    "text = spam[\"text\"]\n",
    "label = spam[\"type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:09.607087Z",
     "start_time": "2022-04-01T01:23:09.596074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:09.638070Z",
     "start_time": "2022-04-01T01:23:09.614083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:09.701072Z",
     "start_time": "2022-04-01T01:23:09.643076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4827\n",
       "spam     747\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Cleaning\n",
    "- 정답의 문자를 숫자로 변환 -> ham은 0으로, spam은 1로 변환\n",
    "- text를 문자만 존재하도록 정리\n",
    "- 대문자들을 모두 소문자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:09.748682Z",
     "start_time": "2022-04-01T01:23:09.704073Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4827\n",
       "1     747\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = label.map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:09.812203Z",
     "start_time": "2022-04-01T01:23:09.754205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat\n"
     ]
    }
   ],
   "source": [
    "re_pattern = \"[^a-zA-Z0-9\\ ]\"\n",
    "\n",
    "# 영어,숫자, 띄어쓰기 제외 문자 제거전\n",
    "print(text[0])\n",
    "\n",
    "# 문자 제거후\n",
    "text = text.str.replace(re_pattern, \"\", regex=True)\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:09.860201Z",
     "start_time": "2022-04-01T01:23:09.819203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat\n",
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n"
     ]
    }
   ],
   "source": [
    "# 소문자 변환전\n",
    "print(text[0])\n",
    "\n",
    "# 소문자 변환후\n",
    "text = text.str.lower()\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:10.959929Z",
     "start_time": "2022-04-01T01:23:09.862214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data size: 3901, 0.70\n",
      "test_data size: 1673, 0.30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_text, test_text, train_label, test_label = train_test_split(\n",
    "    text, label, train_size=0.7, random_state=2021)\n",
    "\n",
    "print(f\"train_data size: {len(train_label)}, {len(train_label)/len(text):.2f}\")\n",
    "print(f\"test_data size: {len(test_label)}, {len(test_label)/len(text):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Count Vectorize\n",
    "- Naive Bayes를 학습시키기 위해서 각 문장에서 단어들이 몇 번 나왔는지로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 word tokenize\n",
    "- 문장을 단어로 나누는 데에는 `nltk` 패키지의 `word_tokenize`를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:12.652447Z",
     "start_time": "2022-04-01T01:23:10.963932Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\inp01\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:12.715994Z",
     "start_time": "2022-04-01T01:23:12.658450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am only searching for good dual sim mobile pa\n",
      "['am', 'only', 'searching', 'for', 'good', 'dual', 'sim', 'mobile', 'pa']\n"
     ]
    }
   ],
   "source": [
    "print(train_text.iloc[0])\n",
    "print(word_tokenize(train_text.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 count vectorize\n",
    "- `CountVectorizer`를 이용해 단어들을 count vector로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:12.747983Z",
     "start_time": "2022-04-01T01:23:12.720981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['am only searching for good dual sim mobile pa',\n",
       "       'excellent ill see what rileys plans are'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_text.iloc[:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:12.779993Z",
     "start_time": "2022-04-01T01:23:12.752982Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inp01\\Anaconda3\\envs\\ojt\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(tokenizer=<function word_tokenize at 0x000001DDD217B0D0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예시로 2개 문장 CountVectorizer를 학습\n",
    "cnt_vectorizer = CountVectorizer(tokenizer=word_tokenize)\n",
    "cnt_vectorizer.fit(train_text.iloc[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:12.811982Z",
     "start_time": "2022-04-01T01:23:12.793009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 0,\n",
       " 'only': 8,\n",
       " 'searching': 12,\n",
       " 'for': 4,\n",
       " 'good': 5,\n",
       " 'dual': 2,\n",
       " 'sim': 14,\n",
       " 'mobile': 7,\n",
       " 'pa': 9,\n",
       " 'excellent': 3,\n",
       " 'ill': 6,\n",
       " 'see': 13,\n",
       " 'what': 15,\n",
       " 'rileys': 11,\n",
       " 'plans': 10,\n",
       " 'are': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:12.843015Z",
     "start_time": "2022-04-01T01:23:12.815983Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['am',\n",
       " 'are',\n",
       " 'dual',\n",
       " 'excellent',\n",
       " 'for',\n",
       " 'good',\n",
       " 'ill',\n",
       " 'mobile',\n",
       " 'only',\n",
       " 'pa',\n",
       " 'plans',\n",
       " 'rileys',\n",
       " 'searching',\n",
       " 'see',\n",
       " 'sim',\n",
       " 'what']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(cnt_vectorizer.vocabulary_.items(), key=lambda x: x[1])\n",
    "vocab = list(map(lambda x: x[0], vocab))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:12.873994Z",
     "start_time": "2022-04-01T01:23:12.848989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_cnt_vector = cnt_vectorizer.transform(train_text.iloc[:2]).toarray()\n",
    "sample_cnt_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:12.889980Z",
     "start_time": "2022-04-01T01:23:12.878982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['am only searching for good dual sim mobile pa',\n",
       "       'excellent ill see what rileys plans are'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.iloc[:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:12.935981Z",
     "start_time": "2022-04-01T01:23:12.893983Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>am</th>\n",
       "      <th>are</th>\n",
       "      <th>dual</th>\n",
       "      <th>excellent</th>\n",
       "      <th>for</th>\n",
       "      <th>good</th>\n",
       "      <th>ill</th>\n",
       "      <th>mobile</th>\n",
       "      <th>only</th>\n",
       "      <th>pa</th>\n",
       "      <th>plans</th>\n",
       "      <th>rileys</th>\n",
       "      <th>searching</th>\n",
       "      <th>see</th>\n",
       "      <th>sim</th>\n",
       "      <th>what</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   am  are  dual  excellent  for  good  ill  mobile  only  pa  plans  rileys  \\\n",
       "0   1    0     1          0    1     1    0       1     1   1      0       0   \n",
       "1   0    1     0          1    0     0    1       0     0   0      1       1   \n",
       "\n",
       "   searching  see  sim  what  \n",
       "0          1    0    1     0  \n",
       "1          0    1    0     1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sample_cnt_vector, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:13.719315Z",
     "start_time": "2022-04-01T01:23:12.938983Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inp01\\Anaconda3\\envs\\ojt\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(tokenizer=<function word_tokenize at 0x000001DDD217B0D0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 데이터 진행\n",
    "cnt_vectorizer = CountVectorizer(tokenizer=word_tokenize)\n",
    "cnt_vectorizer.fit(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:13.735808Z",
     "start_time": "2022-04-01T01:23:13.720514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 단어 개수 : 7908\n"
     ]
    }
   ],
   "source": [
    "print('전체 단어 개수 : {}'.format(len(cnt_vectorizer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Predict\n",
    "- CountVectorize는 학습한 단어장에 존재하지 않는 단어가 들어오게 될 경우 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:14.840673Z",
     "start_time": "2022-04-01T01:23:13.738807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix = cnt_vectorizer.transform(train_text)\n",
    "test_matrix = cnt_vectorizer.transform(test_text)\n",
    "\n",
    "cnt_vectorizer.transform([\"notavailblewordforcnt\"]).toarray().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Naive Bayes\n",
    "- 분류를 위한 Naive Bayes 모델은 `sklearn.naive_bayes`의 `BernoulliNB`를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:14.875692Z",
     "start_time": "2022-04-01T01:23:14.850673Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "naive_bayes = BernoulliNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:14.921702Z",
     "start_time": "2022-04-01T01:23:14.885673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes.fit(train_matrix, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:14.949674Z",
     "start_time": "2022-04-01T01:23:14.928677Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pred = naive_bayes.predict(train_matrix)\n",
    "test_pred = naive_bayes.predict(test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T01:23:14.980773Z",
     "start_time": "2022-04-01T01:23:14.952676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy is 0.9854\n",
      "Test Accuracy is 0.9767\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_acc = accuracy_score(train_label, train_pred)\n",
    "test_acc = accuracy_score(test_label, test_pred)\n",
    "\n",
    "print(f\"Train Accuracy is {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy is {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "ojt",
   "language": "python",
   "name": "ojt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
